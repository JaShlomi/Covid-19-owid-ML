{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff8c31-5b0d-4355-a936-804228203886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('df_cleaned.pkl')\n",
    "\n",
    "# Now 'df' contains the data from the pickled file.\n",
    "# You can verify this by printing the first few rows:\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed5a99-beaf-4815-9a9f-4dae695b4144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963b31e-e246-4ab5-b39a-0669647a9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: pip install (Run this in a Jupyter Notebook cell)\n",
    "\n",
    "!pip install pandas scikit-learn numpy xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18417e5-4c04-429e-9c78-e72a3cd93d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_covid_data(df):\n",
    "    \"\"\"Prepares the COVID-19 DataFrame for time-series forecasting.\"\"\"\n",
    "    print(\"Preparing COVID-19 data...\")\n",
    "\n",
    "    # Calculate the 7-day change in total cases.\n",
    "    df['total_cases_change_7'] = df.groupby('location')['total_cases'].shift(1).diff(7).fillna(0)\n",
    "\n",
    "    # Use the original target variable.\n",
    "    df['target'] = df['new_cases_smoothed_per_million']\n",
    "\n",
    "    # Extract time-based features.\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['week'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "\n",
    "    # Drop rows with NaN due to total_cases_change_7 creation.\n",
    "    df = df.dropna(subset=['total_cases_change_7'])\n",
    "\n",
    "    # Drop total cases and total cases per million to prevent data leakage.\n",
    "    df = df.drop(columns=['total_cases', 'total_cases_per_million'])\n",
    "\n",
    "    # Drop all new cases features except target.\n",
    "    columns_to_drop = [col for col in df.columns if 'new_cases' in col and col != 'new_cases_smoothed_per_million']\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    print(\"COVID-19 data prepared.\")\n",
    "    return df\n",
    "\n",
    "# Assuming 'df' is already loaded in your Jupyter Notebook.\n",
    "\n",
    "# Ensure the 'date' column is in datetime format.\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(\"Starting data preparation...\")\n",
    "df = prepare_covid_data(df.copy())\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "print(\"Starting data scaling...\")\n",
    "# Prepare data for modeling (Corrected X definition)\n",
    "X = df.drop(['target', 'date', 'location', 'new_cases_smoothed_per_million'], axis=1) # new_cases_smoothed_per_million is the target.\n",
    "y = df['target']\n",
    "\n",
    "# Scale the data.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Data scaling complete.\")\n",
    "\n",
    "print(\"Starting feature selection with Lasso...\")\n",
    "# Perform feature selection.\n",
    "lasso = Lasso(alpha=0.01).fit(X_scaled, y)\n",
    "lasso_selected = (np.abs(lasso.coef_) > 0).astype(int)\n",
    "print(\"Lasso feature selection complete.\")\n",
    "\n",
    "print(\"Starting feature selection with Ridge...\")\n",
    "ridge = Ridge(alpha=0.01).fit(X_scaled, y)\n",
    "ridge_selected = (np.abs(ridge.coef_) > 0).astype(int)\n",
    "print(\"Ridge feature selection complete.\")\n",
    "\n",
    "print(\"Starting feature selection with Gradient Boosting...\")\n",
    "gb = GradientBoostingRegressor().fit(X_scaled, y)\n",
    "gb_selected = (gb.feature_importances_ > 0).astype(int)\n",
    "print(\"Gradient Boosting feature selection complete.\")\n",
    "\n",
    "print(\"Starting feature selection with Random Forest...\")\n",
    "rf = RandomForestRegressor().fit(X_scaled, y)\n",
    "rf_selected = (rf.feature_importances_ > 0).astype(int)\n",
    "print(\"Random Forest feature selection complete.\")\n",
    "\n",
    "print(\"Creating feature selection DataFrame...\")\n",
    "selection_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Lasso': lasso_selected,\n",
    "    'GradientBoost': gb_selected,\n",
    "    'RandomForest': rf_selected,\n",
    "    'Ridge': ridge_selected\n",
    "})\n",
    "\n",
    "selection_df['Sum'] = selection_df[['Lasso', 'GradientBoost', 'RandomForest', 'Ridge']].sum(axis=1)\n",
    "\n",
    "print(selection_df)\n",
    "print(\"Feature selection DataFrame created.\")\n",
    "\n",
    "# Save feature selection table to a text file.\n",
    "selection_df.to_csv('feature_selection_table.txt', sep='\\t', index=False)\n",
    "print(\"Feature selection table saved to 'feature_selection_table.txt'\")\n",
    "\n",
    "print(\"Selecting final variables...\")\n",
    "# Select final variables.\n",
    "final_var = selection_df[selection_df['Sum'] >= 4]['Feature'].tolist()\n",
    "df_model = df[final_var].copy()\n",
    "df_model['target'] = df['target'].copy()\n",
    "\n",
    "print(\"Final variables selected.\")\n",
    "print(\"Final DataFrame info:\")\n",
    "df_model.info()\n",
    "print(\"Process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f331d-6af9-4be0-bf60-ce941aa6cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "def prepare_covid_data(df):\n",
    "    \"\"\"Prepares the COVID-19 DataFrame for time-series forecasting.\"\"\"\n",
    "    print(\"Preparing COVID-19 data...\")\n",
    "\n",
    "    # Calculate the 7-day change in total cases.\n",
    "    df['total_cases_change_7'] = df.groupby('location')['total_cases'].shift(1).diff(7).fillna(0)\n",
    "\n",
    "    # Use the original target variable.\n",
    "    df['target'] = df['new_cases_smoothed_per_million']\n",
    "\n",
    "    # Extract time-based features.\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['week'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "\n",
    "    # Drop rows with NaN due to total_cases_change_7 creation.\n",
    "    df = df.dropna(subset=['total_cases_change_7'])\n",
    "\n",
    "    # Drop total cases and total cases per million to prevent data leakage.\n",
    "    df = df.drop(columns=['total_cases', 'total_cases_per_million'])\n",
    "\n",
    "    # Drop all new cases features except target.\n",
    "    columns_to_drop = [col for col in df.columns if 'new_cases' in col and col != 'new_cases_smoothed_per_million']\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    print(\"COVID-19 data prepared.\")\n",
    "    return df\n",
    "\n",
    "# Assuming 'df' is already loaded in your Jupyter Notebook.\n",
    "\n",
    "# Ensure the 'date' column is in datetime format.\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(\"Starting data preparation...\")\n",
    "df = prepare_covid_data(df.copy())\n",
    "print(\"Data preparation complete.\")\n",
    "\n",
    "print(\"Starting data scaling...\")\n",
    "# Prepare data for modeling (Corrected X definition)\n",
    "X = df.drop(['target', 'date', 'location', 'new_cases_smoothed_per_million'], axis=1) # new_cases_smoothed_per_million is the target.\n",
    "y = df['target']\n",
    "\n",
    "# Scale the data.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(\"Data scaling complete.\")\n",
    "\n",
    "print(\"Starting feature selection with Lasso...\")\n",
    "# Perform feature selection.\n",
    "lasso = Lasso(alpha=0.01).fit(X_scaled, y)\n",
    "lasso_selected = (np.abs(lasso.coef_) > 0).astype(int)\n",
    "print(\"Lasso feature selection complete.\")\n",
    "\n",
    "print(\"Starting feature selection with Ridge...\")\n",
    "ridge = Ridge(alpha=0.01).fit(X_scaled, y)\n",
    "ridge_selected = (np.abs(ridge.coef_) > 0).astype(int)\n",
    "print(\"Ridge feature selection complete.\")\n",
    "\n",
    "print(\"Starting feature selection with Gradient Boosting...\")\n",
    "gb = GradientBoostingRegressor().fit(X_scaled, y)\n",
    "gb_selected = (gb.feature_importances_ > 0).astype(int)\n",
    "print(\"Gradient Boosting feature selection complete.\")\n",
    "\n",
    "print(\"Starting feature selection with Random Forest...\")\n",
    "rf = RandomForestRegressor().fit(X_scaled, y)\n",
    "rf_selected = (rf.feature_importances_ > 0).astype(int)\n",
    "print(\"Random Forest feature selection complete.\")\n",
    "\n",
    "print(\"Creating feature selection DataFrame...\")\n",
    "selection_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Lasso': lasso_selected,\n",
    "    'GradientBoost': gb_selected,\n",
    "    'RandomForest': rf_selected,\n",
    "    'Ridge': ridge_selected\n",
    "})\n",
    "\n",
    "selection_df['Sum'] = selection_df[['Lasso', 'GradientBoost', 'RandomForest', 'Ridge']].sum(axis=1)\n",
    "\n",
    "print(selection_df)\n",
    "print(\"Feature selection DataFrame created.\")\n",
    "\n",
    "# Save feature selection table to a text file.\n",
    "selection_df.to_csv('feature_selection_table.txt', sep='\\t', index=False)\n",
    "print(\"Feature selection table saved to 'feature_selection_table.txt'\")\n",
    "\n",
    "print(\"Selecting final variables...\")\n",
    "# Select final variables.\n",
    "final_var = selection_df[selection_df['Sum'] >= 4]['Feature'].tolist()\n",
    "df_model = df[final_var].copy()\n",
    "df_model['target'] = df['target'].copy()\n",
    "df_model['date'] = df['date'].copy()\n",
    "df_model['location'] = df['location'].copy()\n",
    "df_model['continent'] = df['continent'].copy()\n",
    "\n",
    "print(\"Final variables selected.\")\n",
    "print(\"Final DataFrame info:\")\n",
    "df_model.info()\n",
    "\n",
    "print(\"Saving df_model to pickle file...\")\n",
    "# Save df_model to a pickle file.\n",
    "with open('df_model.pkl', 'wb') as f:\n",
    "    pickle.dump(df_model, f)\n",
    "print(\"df_model saved to 'df_model.pkl'\")\n",
    "\n",
    "print(\"Process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b9c6c-95ec-43bd-845f-f29f0c6782c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.drop('location', axis=1, errors='ignore')\n",
    "df_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f3f26-e57e-4291-9290-8d7eaa9c8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import io\n",
    "from sklearn.dummy import DummyRegressor # Added import\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"Calculates the Root Mean Squared Logarithmic Error.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    log_diff = np.log1p(y_true) - np.log1p(y_pred)\n",
    "    return np.sqrt(np.mean(log_diff**2))\n",
    "\n",
    "def evaluate_model(model, X, y, name, results, dataset_type):\n",
    "    \"\"\"Evaluates the model and stores the metrics.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    rmsle_val = rmsle(y, y_pred)\n",
    "\n",
    "    if name not in results:\n",
    "        results[name] = {}\n",
    "\n",
    "    results[name][dataset_type] = {\n",
    "        'MSE': mse, 'MAE': mae, 'R2': r2, \"RMSE\": rmse, \"RMSLE\": rmsle_val\n",
    "    }\n",
    "    print(f\"  Evaluated {name} on {dataset_type}.\")\n",
    "    return f\"  Evaluated {name} on {dataset_type}.\\n    MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}, RMSE: {rmse:.4f}, RMSLE: {rmsle_val:.4f}\\n\"\n",
    "\n",
    "def train_and_evaluate_models(df, start_year, start_month, end_year, end_month):\n",
    "    \"\"\"Trains and evaluates multiple regression models, with stratified location split.\"\"\"\n",
    "\n",
    "    print(\"Starting model training and evaluation...\")\n",
    "\n",
    "    # Debugging: Print min/max year and month values\n",
    "    print(\"Min Year:\", df['year'].min(), \"Max Year:\", df['year'].max())\n",
    "    print(\"Min Month:\", df['month'].min(), \"Max Month:\", df['month'].max())\n",
    "\n",
    "    # Filter data for given period.\n",
    "    df = df[((df['year'] == start_year) & (df['month'] >= start_month)) | ((df['year'] == end_year) & (df['month'] <= end_month))].copy()\n",
    "\n",
    "    print(f\"Data filtered for year {start_year} to {end_year} and month {start_month} to {end_month}. Shape:\", df.shape)\n",
    "\n",
    "    # Check for empty DataFrame after filtering.\n",
    "    if df.empty:\n",
    "        print(\"Error: DataFrame is empty after filtering. Adjust date range.\")\n",
    "        return None  # Or raise an exception, or return a default value\n",
    "\n",
    "    # Extract week based feature.\n",
    "    df['week'] = df['week'].astype(int)\n",
    "\n",
    "    # Location encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['location'] = label_encoder.fit_transform(df['location'])\n",
    "    df['continent'] = df['continent'].astype(int)\n",
    "\n",
    "    # Stratified location split (KEEP DATE COLUMN DURING SPLIT)\n",
    "    unique_locations = df['location'].unique()\n",
    "    train_locations, temp_locations = train_test_split(unique_locations, test_size=0.3, random_state=42, stratify=df.groupby('location')['continent'].first().loc[unique_locations])\n",
    "    val_locations, test_locations = train_test_split(temp_locations, test_size=0.5, random_state=42, stratify=df.groupby('location')['continent'].first().loc[temp_locations])\n",
    "    print(\"Split locations into train/val/test.\")\n",
    "\n",
    "    train_df = df[df['location'].isin(train_locations)]\n",
    "    val_df = df[df['location'].isin(val_locations)]\n",
    "    test_df = df[df['location'].isin(test_locations)]\n",
    "    print(\"Split data into train/val/test. Train shape:\", train_df.shape, \"Val shape:\", val_df.shape, \"Test shape:\", test_df.shape)\n",
    "\n",
    "    # Use the selected features from df_model.\n",
    "    selected_features = [col for col in df.columns if col not in ['target', 'location', 'continent', 'year', 'month','date','iso_code']]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['target']\n",
    "    X_val = val_df[selected_features]\n",
    "    y_val = val_df['target']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    print(\"Prepared X and y train/val/test.\")\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Scaled features.\")\n",
    "\n",
    "    # Define models\n",
    "    models = {\n",
    "        'Dummy Regressor (Mean)': DummyRegressor(strategy='mean'),\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest Regressor': RandomForestRegressor(random_state=42),\n",
    "        'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),\n",
    "        'SVR': SVR(),\n",
    "        'K-Nearest Neighbors Regressor': KNeighborsRegressor(),\n",
    "        'XGBoost Regressor': xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    summary_text = \"Model Comparison:\\n\"\n",
    "\n",
    "    # Train and evaluate models\n",
    "    print(\"Starting model training loop...\")\n",
    "    for name, model in models.items():\n",
    "        print(f\"  Training {name}...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        summary_text += evaluate_model(model, X_train_scaled, y_train, name, results, 'train')\n",
    "        summary_text += evaluate_model(model, X_val_scaled, y_val, name, results, 'val')\n",
    "        summary_text += evaluate_model(model, X_test_scaled, y_test, name, results, 'test')\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(summary_text)\n",
    "\n",
    "    # Save summary to local file\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    print(\"Model summary saved to local file: model_summary.txt\")\n",
    "\n",
    "    print(\"Model training and evaluation complete.\")\n",
    "    return models\n",
    "\n",
    "# Example usage (assuming your DataFrame is called 'df_model' and already loaded)\n",
    "#df_model['date'] = pd.to_datetime(df_model['date']) #date column not in df_model, but in df.\n",
    "\n",
    "# Set the desired period\n",
    "start_year = 2021\n",
    "start_month = 11\n",
    "end_year = 2022\n",
    "end_month = 4\n",
    "\n",
    "trained_models = train_and_evaluate_models(df_model.copy(), start_year, start_month, end_year, end_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59722a-8916-4567-9d2e-89e1a9e607f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import io\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"Calculates the Root Mean Squared Logarithmic Error.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    log_diff = np.log1p(y_true) - np.log1p(y_pred)\n",
    "    return np.sqrt(np.mean(log_diff**2))\n",
    "\n",
    "def evaluate_model(model, X, y, name, results, dataset_type):\n",
    "    \"\"\"Evaluates the model and stores the metrics, clipping negative predictions.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_clipped = np.maximum(y_pred, 0) # Clip negative predictions\n",
    "\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    rmsle_val = rmsle(y, y_pred_clipped)\n",
    "\n",
    "    if name not in results:\n",
    "        results[name] = {}\n",
    "\n",
    "    results[name][dataset_type] = {\n",
    "        'MSE': mse, 'MAE': mae, 'R2': r2, \"RMSE\": rmse, \"RMSLE\": rmsle_val\n",
    "    }\n",
    "    print(f\"  Evaluated {name} on {dataset_type}.\")\n",
    "    return f\"  Evaluated {name} on {dataset_type}.\\n    MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}, RMSE: {rmse:.4f}, RMSLE: {rmsle_val:.4f}\\n\"\n",
    "\n",
    "def train_and_evaluate_models(df, start_year, start_month, end_year, end_month):\n",
    "    \"\"\"Trains and evaluates multiple regression models, with stratified iso_code split.\"\"\"\n",
    "\n",
    "    print(\"Starting model training and evaluation...\")\n",
    "\n",
    "    # Debugging: Print min/max year and month values\n",
    "    print(\"Min Year:\", df['year'].min(), \"Max Year:\", df['year'].max())\n",
    "    print(\"Min Month:\", df['month'].min(), \"Max Month:\", df['month'].max())\n",
    "\n",
    "    # Filter data for given period.\n",
    "    df = df[((df['year'] == start_year) & (df['month'] >= start_month)) | ((df['year'] == end_year) & (df['month'] <= end_month))].copy()\n",
    "\n",
    "    print(f\"Data filtered for year {start_year} to {end_year} and month {start_month} to {end_month}. Shape:\", df.shape)\n",
    "\n",
    "    # Check for empty DataFrame after filtering.\n",
    "    if df.empty:\n",
    "        print(\"Error: DataFrame is empty after filtering. Adjust date range.\")\n",
    "        return None\n",
    "\n",
    "    # Extract week based feature.\n",
    "    df['week'] = df['week'].astype(int)\n",
    "\n",
    "    # iso_code encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['iso_code'] = label_encoder.fit_transform(df['iso_code'])\n",
    "    df['continent'] = df['continent'].astype(int)\n",
    "\n",
    "    # Stratified iso_code split\n",
    "    unique_iso_codes = df['iso_code'].unique()\n",
    "    train_iso_codes, temp_iso_codes = train_test_split(unique_iso_codes, test_size=0.3, random_state=42, stratify=df.groupby('iso_code')['continent'].first().loc[unique_iso_codes])\n",
    "    val_iso_codes, test_iso_codes = train_test_split(temp_iso_codes, test_size=0.5, random_state=42, stratify=df.groupby('iso_code')['continent'].first().loc[temp_iso_codes])\n",
    "    print(\"Split iso_codes into train/val/test.\")\n",
    "\n",
    "    train_df = df[df['iso_code'].isin(train_iso_codes)]\n",
    "    val_df = df[df['iso_code'].isin(val_iso_codes)]\n",
    "    test_df = df[df['iso_code'].isin(test_iso_codes)]\n",
    "    print(\"Split data into train/val/test. Train shape:\", train_df.shape, \"Val shape:\", val_df.shape, \"Test shape:\", test_df.shape)\n",
    "\n",
    "    # Use the selected features.\n",
    "    selected_features = [col for col in df.columns if col not in ['target', 'iso_code', 'continent', 'year', 'month', 'date', 'location']]\n",
    "\n",
    "    X_train = train_df[selected_features]\n",
    "    y_train = train_df['target']\n",
    "    X_val = val_df[selected_features]\n",
    "    y_val = val_df['target']\n",
    "    X_test = test_df[selected_features]\n",
    "    y_test = test_df['target']\n",
    "\n",
    "    print(\"Prepared X and y train/val/test.\")\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Scaled features.\")\n",
    "\n",
    "    # Define models\n",
    "    models = {\n",
    "        'Dummy Regressor (Mean)': DummyRegressor(strategy='mean'),\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest Regressor': RandomForestRegressor(random_state=42),\n",
    "        'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),\n",
    "        'SVR': SVR(),\n",
    "        'K-Nearest Neighbors Regressor': KNeighborsRegressor(),\n",
    "        'XGBoost Regressor': xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    summary_text = \"Model Comparison:\\n\"\n",
    "\n",
    "    # Train and evaluate models\n",
    "    print(\"Starting model training loop...\")\n",
    "    for name, model in models.items():\n",
    "        print(f\"  Training {name}...\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        summary_text += evaluate_model(model, X_train_scaled, y_train, name, results, 'train')\n",
    "        summary_text += evaluate_model(model, X_val_scaled, y_val, name, results, 'val')\n",
    "        summary_text += evaluate_model(model, X_test_scaled, y_test, name, results, 'test')\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(summary_text)\n",
    "\n",
    "    # Save summary to local file\n",
    "    with open('model_summary.txt', 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    print(\"Model summary saved to local file: model_summary.txt\")\n",
    "\n",
    "    print(\"Model training and evaluation complete.\")\n",
    "    return models\n",
    "\n",
    "# Example usage (assuming your DataFrame is called 'df_model' and already loaded)\n",
    "#df_model['date'] = pd.to_datetime(df_model['date']) #date column not in df_model, but in df.\n",
    "\n",
    "# Set the desired period\n",
    "start_year = 2021\n",
    "start_month = 11\n",
    "end_year = 2022\n",
    "end_month = 4\n",
    "\n",
    "trained_models = train_and_evaluate_models(df_model.copy(), start_year, start_month, end_year, end_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cec8880-2183-49f0-9d4f-0a6f5a2b5ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training and evaluation with cross-validation...\n",
      "Data filtered for year 2021 to 2022 and month 11 to 4. Shape: (44102, 28)\n",
      "\n",
      "Fold 1/5\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "  Evaluated Linear Regression on train.\n",
      "  Evaluated Linear Regression on val.\n",
      "  Evaluated Linear Regression on test.\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "  Evaluated SVR on train.\n",
      "  Evaluated SVR on val.\n",
      "  Evaluated SVR on test.\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "Fold 1:\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "    MSE: 6.8995, MAE: 2.3007, R2: 0.0000, RMSE: 2.6267, RMSLE: 0.7481\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "    MSE: 7.5480, MAE: 2.4059, R2: -0.0510, RMSE: 2.7474, RMSLE: 0.7356\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "    MSE: 6.6683, MAE: 2.2398, R2: -0.0021, RMSE: 2.5823, RMSLE: 0.7137\n",
      "  Evaluated Linear Regression on train.\n",
      "    MSE: 1.5546, MAE: 0.9393, R2: 0.7747, RMSE: 1.2468, RMSLE: 0.3649\n",
      "  Evaluated Linear Regression on val.\n",
      "    MSE: 2.3227, MAE: 1.1617, R2: 0.6766, RMSE: 1.5240, RMSLE: 0.4049\n",
      "  Evaluated Linear Regression on test.\n",
      "    MSE: 3.0554, MAE: 1.2810, R2: 0.5408, RMSE: 1.7480, RMSLE: 0.5063\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "    MSE: 0.0088, MAE: 0.0256, R2: 0.9987, RMSE: 0.0940, RMSLE: 0.0350\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "    MSE: 1.2669, MAE: 0.7112, R2: 0.8236, RMSE: 1.1256, RMSLE: 0.2407\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "    MSE: 0.9690, MAE: 0.7075, R2: 0.8544, RMSE: 0.9844, RMSLE: 0.2716\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "    MSE: 0.2400, MAE: 0.3182, R2: 0.9652, RMSE: 0.4899, RMSLE: 0.1662\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "    MSE: 0.7928, MAE: 0.6185, R2: 0.8896, RMSE: 0.8904, RMSLE: 0.2140\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "    MSE: 0.7281, MAE: 0.6200, R2: 0.8906, RMSE: 0.8533, RMSLE: 0.2682\n",
      "  Evaluated SVR on train.\n",
      "    MSE: 0.2221, MAE: 0.2756, R2: 0.9678, RMSE: 0.4713, RMSLE: 0.1601\n",
      "  Evaluated SVR on val.\n",
      "    MSE: 1.1552, MAE: 0.8432, R2: 0.8391, RMSE: 1.0748, RMSLE: 0.2870\n",
      "  Evaluated SVR on test.\n",
      "    MSE: 1.6799, MAE: 1.0058, R2: 0.7476, RMSE: 1.2961, RMSLE: 0.4161\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "    MSE: 0.0248, MAE: 0.0471, R2: 0.9964, RMSE: 0.1575, RMSLE: 0.0499\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "    MSE: 2.7046, MAE: 1.1663, R2: 0.6234, RMSE: 1.6446, RMSLE: 0.3945\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "    MSE: 2.8902, MAE: 1.2518, R2: 0.5657, RMSE: 1.7001, RMSLE: 0.4712\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "    MSE: 0.0318, MAE: 0.1019, R2: 0.9954, RMSE: 0.1783, RMSLE: 0.0570\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "    MSE: 0.8063, MAE: 0.6053, R2: 0.8877, RMSE: 0.8980, RMSLE: 0.2015\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "    MSE: 0.8599, MAE: 0.6358, R2: 0.8708, RMSE: 0.9273, RMSLE: 0.2622\n",
      "\n",
      "Fold 2/5\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "  Evaluated Linear Regression on train.\n",
      "  Evaluated Linear Regression on val.\n",
      "  Evaluated Linear Regression on test.\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "  Evaluated SVR on train.\n",
      "  Evaluated SVR on val.\n",
      "  Evaluated SVR on test.\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "Fold 2:\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "    MSE: 6.9065, MAE: 2.2937, R2: 0.0000, RMSE: 2.6280, RMSLE: 0.7457\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "    MSE: 6.3732, MAE: 2.2059, R2: -0.0009, RMSE: 2.5245, RMSLE: 0.6928\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "    MSE: 7.6799, MAE: 2.4542, R2: -0.0058, RMSE: 2.7713, RMSLE: 0.8234\n",
      "  Evaluated Linear Regression on train.\n",
      "    MSE: 1.8571, MAE: 1.0291, R2: 0.7311, RMSE: 1.3628, RMSLE: 0.4027\n",
      "  Evaluated Linear Regression on val.\n",
      "    MSE: 1.1944, MAE: 0.8453, R2: 0.8124, RMSE: 1.0929, RMSLE: 0.3010\n",
      "  Evaluated Linear Regression on test.\n",
      "    MSE: 1.5931, MAE: 0.9258, R2: 0.7914, RMSE: 1.2622, RMSLE: 0.4088\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "    MSE: 0.0095, MAE: 0.0262, R2: 0.9986, RMSE: 0.0973, RMSLE: 0.0355\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "    MSE: 0.5223, MAE: 0.5137, R2: 0.9180, RMSE: 0.7227, RMSLE: 0.1925\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "    MSE: 0.6441, MAE: 0.5045, R2: 0.9156, RMSE: 0.8026, RMSLE: 0.2399\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "    MSE: 0.2367, MAE: 0.3161, R2: 0.9657, RMSE: 0.4865, RMSLE: 0.1639\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "    MSE: 0.4112, MAE: 0.4634, R2: 0.9354, RMSE: 0.6413, RMSLE: 0.1921\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "    MSE: 0.8895, MAE: 0.5932, R2: 0.8835, RMSE: 0.9431, RMSLE: 0.3005\n",
      "  Evaluated SVR on train.\n",
      "    MSE: 0.2360, MAE: 0.2920, R2: 0.9658, RMSE: 0.4858, RMSLE: 0.1600\n",
      "  Evaluated SVR on val.\n",
      "    MSE: 0.8046, MAE: 0.6733, R2: 0.8736, RMSE: 0.8970, RMSLE: 0.2757\n",
      "  Evaluated SVR on test.\n",
      "    MSE: 1.2391, MAE: 0.8579, R2: 0.8377, RMSE: 1.1131, RMSLE: 0.3370\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "    MSE: 0.0276, MAE: 0.0487, R2: 0.9960, RMSE: 0.1660, RMSLE: 0.0519\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "    MSE: 2.0932, MAE: 1.0870, R2: 0.6713, RMSE: 1.4468, RMSLE: 0.3965\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "    MSE: 2.2355, MAE: 1.0255, R2: 0.7072, RMSE: 1.4952, RMSLE: 0.4111\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "    MSE: 0.0317, MAE: 0.1018, R2: 0.9954, RMSE: 0.1781, RMSLE: 0.0564\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "    MSE: 0.5190, MAE: 0.5204, R2: 0.9185, RMSE: 0.7204, RMSLE: 0.2022\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "    MSE: 0.6523, MAE: 0.5173, R2: 0.9146, RMSE: 0.8077, RMSLE: 0.2460\n",
      "\n",
      "Fold 3/5\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "  Evaluated Linear Regression on train.\n",
      "  Evaluated Linear Regression on val.\n",
      "  Evaluated Linear Regression on test.\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "  Evaluated SVR on train.\n",
      "  Evaluated SVR on val.\n",
      "  Evaluated SVR on test.\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "Fold 3:\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "    MSE: 7.0856, MAE: 2.3326, R2: 0.0000, RMSE: 2.6619, RMSLE: 0.7546\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "    MSE: 6.2173, MAE: 2.1434, R2: -0.0035, RMSE: 2.4935, RMSLE: 0.6949\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "    MSE: 6.4502, MAE: 2.2175, R2: -0.0004, RMSE: 2.5397, RMSLE: 0.7522\n",
      "  Evaluated Linear Regression on train.\n",
      "    MSE: 1.6793, MAE: 0.9656, R2: 0.7630, RMSE: 1.2959, RMSLE: 0.3777\n",
      "  Evaluated Linear Regression on val.\n",
      "    MSE: 2.5803, MAE: 1.1535, R2: 0.5835, RMSE: 1.6063, RMSLE: 0.4482\n",
      "  Evaluated Linear Regression on test.\n",
      "    MSE: 2.1993, MAE: 1.1465, R2: 0.6589, RMSE: 1.4830, RMSLE: 0.4139\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "    MSE: 0.0085, MAE: 0.0253, R2: 0.9988, RMSE: 0.0923, RMSLE: 0.0331\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "    MSE: 0.8528, MAE: 0.6035, R2: 0.8624, RMSE: 0.9235, RMSLE: 0.2443\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "    MSE: 1.0920, MAE: 0.7376, R2: 0.8306, RMSE: 1.0450, RMSLE: 0.3842\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "    MSE: 0.2185, MAE: 0.3021, R2: 0.9692, RMSE: 0.4675, RMSLE: 0.1479\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "    MSE: 0.8535, MAE: 0.6409, R2: 0.8622, RMSE: 0.9239, RMSLE: 0.2644\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "    MSE: 0.7353, MAE: 0.6291, R2: 0.8860, RMSE: 0.8575, RMSLE: 0.3086\n",
      "  Evaluated SVR on train.\n",
      "    MSE: 0.2116, MAE: 0.2698, R2: 0.9701, RMSE: 0.4600, RMSLE: 0.1539\n",
      "  Evaluated SVR on val.\n",
      "    MSE: 1.4120, MAE: 0.8697, R2: 0.7721, RMSE: 1.1883, RMSLE: 0.3322\n",
      "  Evaluated SVR on test.\n",
      "    MSE: 1.8134, MAE: 1.0305, R2: 0.7187, RMSE: 1.3466, RMSLE: 0.4570\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "    MSE: 0.0262, MAE: 0.0475, R2: 0.9963, RMSE: 0.1619, RMSLE: 0.0509\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "    MSE: 2.7853, MAE: 1.1827, R2: 0.5505, RMSE: 1.6689, RMSLE: 0.4873\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "    MSE: 2.6276, MAE: 1.2187, R2: 0.5925, RMSE: 1.6210, RMSLE: 0.4820\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "    MSE: 0.0298, MAE: 0.0995, R2: 0.9958, RMSE: 0.1726, RMSLE: 0.0541\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "    MSE: 0.7373, MAE: 0.5942, R2: 0.8810, RMSE: 0.8587, RMSLE: 0.2440\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "    MSE: 1.1134, MAE: 0.7097, R2: 0.8273, RMSE: 1.0552, RMSLE: 0.3788\n",
      "\n",
      "Fold 4/5\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "  Evaluated Linear Regression on train.\n",
      "  Evaluated Linear Regression on val.\n",
      "  Evaluated Linear Regression on test.\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "  Evaluated SVR on train.\n",
      "  Evaluated SVR on val.\n",
      "  Evaluated SVR on test.\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "Fold 4:\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "    MSE: 6.8562, MAE: 2.2811, R2: 0.0000, RMSE: 2.6184, RMSLE: 0.7311\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "    MSE: 6.3832, MAE: 2.1590, R2: -0.1907, RMSE: 2.5265, RMSLE: 0.8181\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "    MSE: 8.2972, MAE: 2.6121, R2: -0.0046, RMSE: 2.8805, RMSLE: 0.8688\n",
      "  Evaluated Linear Regression on train.\n",
      "    MSE: 1.7808, MAE: 0.9982, R2: 0.7403, RMSE: 1.3345, RMSLE: 0.3924\n",
      "  Evaluated Linear Regression on val.\n",
      "    MSE: 1.9878, MAE: 1.0757, R2: 0.6292, RMSE: 1.4099, RMSLE: 0.4468\n",
      "  Evaluated Linear Regression on test.\n",
      "    MSE: 1.5754, MAE: 1.0138, R2: 0.8093, RMSE: 1.2551, RMSLE: 0.4568\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "    MSE: 0.0097, MAE: 0.0265, R2: 0.9986, RMSE: 0.0986, RMSLE: 0.0364\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "    MSE: 1.2451, MAE: 0.7127, R2: 0.7677, RMSE: 1.1158, RMSLE: 0.3934\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "    MSE: 0.4161, MAE: 0.4380, R2: 0.9496, RMSE: 0.6451, RMSLE: 0.2851\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "    MSE: 0.2393, MAE: 0.3185, R2: 0.9651, RMSE: 0.4891, RMSLE: 0.1591\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "    MSE: 0.6847, MAE: 0.5762, R2: 0.8723, RMSE: 0.8274, RMSLE: 0.2772\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "    MSE: 0.2295, MAE: 0.3359, R2: 0.9722, RMSE: 0.4790, RMSLE: 0.2053\n",
      "  Evaluated SVR on train.\n",
      "    MSE: 0.2303, MAE: 0.2902, R2: 0.9664, RMSE: 0.4799, RMSLE: 0.1584\n",
      "  Evaluated SVR on val.\n",
      "    MSE: 1.1100, MAE: 0.7970, R2: 0.7929, RMSE: 1.0536, RMSLE: 0.3281\n",
      "  Evaluated SVR on test.\n",
      "    MSE: 1.1143, MAE: 0.8085, R2: 0.8651, RMSE: 1.0556, RMSLE: 0.3690\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "    MSE: 0.0261, MAE: 0.0479, R2: 0.9962, RMSE: 0.1615, RMSLE: 0.0493\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "    MSE: 3.1179, MAE: 1.2754, R2: 0.4184, RMSE: 1.7657, RMSLE: 0.5390\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "    MSE: 2.2378, MAE: 0.9384, R2: 0.7290, RMSE: 1.4959, RMSLE: 0.3864\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "    MSE: 0.0321, MAE: 0.1033, R2: 0.9953, RMSE: 0.1792, RMSLE: 0.0556\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "    MSE: 0.7083, MAE: 0.5779, R2: 0.8679, RMSE: 0.8416, RMSLE: 0.2851\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "    MSE: 0.4655, MAE: 0.4548, R2: 0.9436, RMSE: 0.6823, RMSLE: 0.3102\n",
      "\n",
      "Fold 5/5\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "  Evaluated Linear Regression on train.\n",
      "  Evaluated Linear Regression on val.\n",
      "  Evaluated Linear Regression on test.\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "  Evaluated SVR on train.\n",
      "  Evaluated SVR on val.\n",
      "  Evaluated SVR on test.\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "Fold 5:\n",
      "  Evaluated Dummy Regressor (Mean) on train.\n",
      "    MSE: 6.8935, MAE: 2.2991, R2: 0.0000, RMSE: 2.6255, RMSLE: 0.7561\n",
      "  Evaluated Dummy Regressor (Mean) on val.\n",
      "    MSE: 6.6878, MAE: 2.2412, R2: -0.1370, RMSE: 2.5861, RMSLE: 0.5830\n",
      "  Evaluated Dummy Regressor (Mean) on test.\n",
      "    MSE: 7.6070, MAE: 2.4248, R2: -0.0029, RMSE: 2.7581, RMSLE: 0.7601\n",
      "  Evaluated Linear Regression on train.\n",
      "    MSE: 1.8216, MAE: 1.0150, R2: 0.7357, RMSE: 1.3497, RMSLE: 0.4075\n",
      "  Evaluated Linear Regression on val.\n",
      "    MSE: 1.5881, MAE: 0.9858, R2: 0.7300, RMSE: 1.2602, RMSLE: 0.2949\n",
      "  Evaluated Linear Regression on test.\n",
      "    MSE: 1.4705, MAE: 0.9329, R2: 0.8061, RMSE: 1.2127, RMSLE: 0.3253\n",
      "  Evaluated Random Forest Regressor on train.\n",
      "    MSE: 0.0094, MAE: 0.0259, R2: 0.9986, RMSE: 0.0971, RMSLE: 0.0368\n",
      "  Evaluated Random Forest Regressor on val.\n",
      "    MSE: 0.3921, MAE: 0.4059, R2: 0.9333, RMSE: 0.6262, RMSLE: 0.1839\n",
      "  Evaluated Random Forest Regressor on test.\n",
      "    MSE: 0.5766, MAE: 0.5472, R2: 0.9240, RMSE: 0.7593, RMSLE: 0.2124\n",
      "  Evaluated Gradient Boosting Regressor on train.\n",
      "    MSE: 0.2704, MAE: 0.3349, R2: 0.9608, RMSE: 0.5200, RMSLE: 0.1739\n",
      "  Evaluated Gradient Boosting Regressor on val.\n",
      "    MSE: 0.2642, MAE: 0.3743, R2: 0.9551, RMSE: 0.5140, RMSLE: 0.1543\n",
      "  Evaluated Gradient Boosting Regressor on test.\n",
      "    MSE: 0.5029, MAE: 0.4858, R2: 0.9337, RMSE: 0.7091, RMSLE: 0.1923\n",
      "  Evaluated SVR on train.\n",
      "    MSE: 0.2240, MAE: 0.2836, R2: 0.9675, RMSE: 0.4733, RMSLE: 0.1600\n",
      "  Evaluated SVR on val.\n",
      "    MSE: 0.9007, MAE: 0.7194, R2: 0.8469, RMSE: 0.9491, RMSLE: 0.2380\n",
      "  Evaluated SVR on test.\n",
      "    MSE: 0.7739, MAE: 0.6612, R2: 0.8980, RMSE: 0.8797, RMSLE: 0.2645\n",
      "  Evaluated K-Nearest Neighbors Regressor on train.\n",
      "    MSE: 0.0265, MAE: 0.0477, R2: 0.9962, RMSE: 0.1628, RMSLE: 0.0515\n",
      "  Evaluated K-Nearest Neighbors Regressor on val.\n",
      "    MSE: 1.9940, MAE: 1.0271, R2: 0.6610, RMSE: 1.4121, RMSLE: 0.3245\n",
      "  Evaluated K-Nearest Neighbors Regressor on test.\n",
      "    MSE: 1.9392, MAE: 0.9982, R2: 0.7443, RMSE: 1.3926, RMSLE: 0.3614\n",
      "  Evaluated XGBoost Regressor on train.\n",
      "    MSE: 0.0316, MAE: 0.1015, R2: 0.9954, RMSE: 0.1777, RMSLE: 0.0553\n",
      "  Evaluated XGBoost Regressor on val.\n",
      "    MSE: 0.2972, MAE: 0.3858, R2: 0.9495, RMSE: 0.5451, RMSLE: 0.1509\n",
      "  Evaluated XGBoost Regressor on test.\n",
      "    MSE: 0.4674, MAE: 0.5178, R2: 0.9384, RMSE: 0.6837, RMSLE: 0.2136\n",
      "\n",
      "Average Model Comparison:\n",
      "Results saved to my_results.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"Calculates the Root Mean Squared Logarithmic Error.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    log_diff = np.log1p(y_true) - np.log1p(y_pred)\n",
    "    return np.sqrt(np.mean(log_diff**2))\n",
    "\n",
    "def evaluate_model(model, X, y, name, results, dataset_type):\n",
    "    \"\"\"Evaluates the model and stores the metrics, clipping negative predictions.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_clipped = np.maximum(y_pred, 0)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    rmsle_val = rmsle(y, y_pred_clipped)\n",
    "\n",
    "    if name not in results:\n",
    "        results[name] = {}\n",
    "\n",
    "    results[name][dataset_type] = {\n",
    "        'MSE': mse, 'MAE': mae, 'R2': r2, \"RMSE\": rmse, \"RMSLE\": rmsle_val\n",
    "    }\n",
    "    print(f\"  Evaluated {name} on {dataset_type}.\")\n",
    "    return f\"  Evaluated {name} on {dataset_type}.\\n    MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}, RMSE: {rmse:.4f}, RMSLE: {rmsle_val:.4f}\\n\"\n",
    "\n",
    "def train_and_evaluate_models_cv(df, start_year, start_month, end_year, end_month, n_splits=5, test_size=0.15, val_size=0.15, output_file=\"results.txt\"):\n",
    "    \"\"\"Trains and evaluates multiple regression models using 5-fold cross-validation and saves results to a file and prints to console.\"\"\"\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"Starting model training and evaluation with cross-validation...\\n\")\n",
    "        print(\"Starting model training and evaluation with cross-validation...\")\n",
    "\n",
    "        # Filter data for given period.\n",
    "        df_filtered = pd.DataFrame()\n",
    "        if start_year == end_year:\n",
    "            df_filtered = df[((df['year'] == start_year) & (df['month'] >= start_month) & (df['month'] <= end_month))].copy()\n",
    "        else:\n",
    "            df_filtered = pd.concat([\n",
    "                df[((df['year'] == start_year) & (df['month'] >= start_month))].copy(),\n",
    "                df[((df['year'] == end_year) & (df['month'] <= end_month))].copy(),\n",
    "                df[((df['year'] > start_year) & (df['year'] < end_year))].copy()\n",
    "            ])\n",
    "        df = df_filtered\n",
    "\n",
    "        f.write(f\"Data filtered for year {start_year} to {end_year} and month {start_month} to {end_month}. Shape: {df.shape}\\n\")\n",
    "        print(f\"Data filtered for year {start_year} to {end_year} and month {start_month} to {end_month}. Shape:\", df.shape)\n",
    "\n",
    "        if df.empty:\n",
    "            f.write(\"Error: DataFrame is empty after filtering. Adjust date range.\\n\")\n",
    "            print(\"Error: DataFrame is empty after filtering. Adjust date range.\")\n",
    "            return None\n",
    "\n",
    "        df['week'] = df['week'].astype(int)\n",
    "        label_encoder = LabelEncoder()\n",
    "        df['iso_code'] = label_encoder.fit_transform(df['iso_code'])\n",
    "        df['continent'] = df['continent'].astype(int)\n",
    "\n",
    "        unique_iso_codes = df['iso_code'].unique()\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        results_cv = {}\n",
    "\n",
    "        for fold, (train_index, val_test_index) in enumerate(kf.split(unique_iso_codes)):\n",
    "            f.write(f\"\\nFold {fold + 1}/{n_splits}\\n\")\n",
    "            print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "            train_iso_codes = unique_iso_codes[train_index]\n",
    "            val_test_iso_codes = unique_iso_codes[val_test_index]\n",
    "\n",
    "            # Split val_test into val and test\n",
    "            val_iso_codes, test_iso_codes = train_test_split(val_test_iso_codes, test_size=val_size / (val_size + test_size), random_state=42)\n",
    "\n",
    "            train_df = df[df['iso_code'].isin(train_iso_codes)]\n",
    "            val_df = df[df['iso_code'].isin(val_iso_codes)]\n",
    "            test_df = df[df['iso_code'].isin(test_iso_codes)]\n",
    "\n",
    "            selected_features = [col for col in df.columns if col not in ['target', 'iso_code', 'continent', 'year', 'month', 'date', 'location']]\n",
    "\n",
    "            X_train, y_train = train_df[selected_features], train_df['target']\n",
    "            X_val, y_val = val_df[selected_features], val_df['target']\n",
    "            X_test, y_test = test_df[selected_features], test_df['target']\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            models = {\n",
    "                'Dummy Regressor (Mean)': DummyRegressor(strategy='mean'),\n",
    "                'Linear Regression': LinearRegression(),\n",
    "                'Random Forest Regressor': RandomForestRegressor(random_state=42),\n",
    "                'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),\n",
    "                'SVR': SVR(),\n",
    "                'K-Nearest Neighbors Regressor': KNeighborsRegressor(),\n",
    "                'XGBoost Regressor': xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "            }\n",
    "\n",
    "            results = {}\n",
    "            summary_text = f\"Fold {fold + 1}:\\n\"\n",
    "\n",
    "            for name, model in models.items():\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                summary_text += evaluate_model(model, X_train_scaled, y_train, name, results, 'train')\n",
    "                summary_text += evaluate_model(model, X_val_scaled, y_val, name, results, 'val')\n",
    "                summary_text += evaluate_model(model, X_test_scaled, y_test, name, results, 'test')\n",
    "\n",
    "            f.write(summary_text)\n",
    "            print(summary_text, end='')\n",
    "            results_cv[fold] = results\n",
    "\n",
    "        # Average results across folds\n",
    "        average_results = {}\n",
    "        f.write(\"\\nAverage Model Comparison:\\n\")\n",
    "        print(\"\\nAverage Model Comparison:\")\n",
    "        for model_name, avg_results in average_results.items():\n",
    "            f.write(f\"\\n{model_name}:\\n\")\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            for dataset_type, metrics in avg_results.items():\n",
    "                f.write(f\"  {dataset_type}: {metrics}\\n\")\n",
    "                print(f\"  {dataset_type}: {metrics}\")\n",
    "\n",
    "        return average_results\n",
    "\n",
    "# Load your data from df_model.pkl\n",
    "try:\n",
    "    df_model = pd.read_pickle(\"df_model.pkl\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: df_model.pkl file not found.\")\n",
    "    exit()\n",
    "\n",
    "# Call the function with df_model and specify the output file\n",
    "start_year = 2021\n",
    "start_month = 11\n",
    "end_year = 2022\n",
    "end_month = 4\n",
    "\n",
    "average_results = train_and_evaluate_models_cv(df_model.copy(), start_year, start_month, end_year, end_month, output_file=\"my_results.txt\")\n",
    "\n",
    "if average_results is not None:\n",
    "    print(\"Results saved to my_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e490f-1e09-4910-8240-d4e3ee7ecc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1cc57-489c-476d-adb8-15a553415e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
